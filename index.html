<!doctype html>
<head>
	<title>CS247 P3: Gesture Controller | Reno Bowen, Nicholas Hippenmeyer, Ilias Karim, Brad Lawson, & Keegan Poppen</title>

	<link rel="stylesheet" href="css/style.css">

	<link href='http://fonts.googleapis.com/css?family=GFS+Didot' rel='stylesheet' type='text/css'>

	<meta name="author" content="Reno Bowen, Nicholas Hippenmeyer, Ilias Karim, Brad Lawson, and Keegan Poppen">
	<meta name="description" content="">
	<meta name="keywords" content="">

	<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
	<meta name="viewport" content="width=device-width,initial-scale=1">

	<!--[if lt IE 9]>
	<script src="http://html5shim.googlecode.com/svn/trunk/html5.js"></script>
	<![endif]-->
</head>

<body>
	<header>
		<h1>
CS247 Project 3: Gesture Controller
		</h1>
		<h2>
Reno Bowen, Nicholas Hippenmeyer, Ilias Karim, Brad Lawson, and Keegan Poppen
		</h2>
		
		<nav>
			<a href="#introduction">
I. Introduction</a>
|
			<a href="#design_process">
II. Design Process</a>
|
			<a href="#implementation">
III. Implementation</a>
|
			<a href="#discussion">
IV. Discussion</a>
		</nav>
	</header>

	<div>

		<a name="introduction">
			<h1>
I. Introduction
			</h1>

			<p>
Designing gestures to select virtual targets via the sophisticated technologies embedded in the Kinect controller and its SDK presents unique problems that we would otherwise not encounter designing and developing interfaces for more conventional interface devices like mice, trackpads, or touchscreens. Best practices for designing gestural interfaces have yet to be established, and the inherent nature of the Kinect and its limitations demand a novel approach and paradigm for user interaction. Though daunting, we regard this challenge as an incredible opportunity to explore and hopefully influence how humans will one day interact with many of their devices--that is, through natural user interfaces, without the need to handle controllers physically.
			</p>
		</a>


		<a name="design_process">

			<h1>
II. Design Process
			</h1>

			<p>
Our group of five coalesced around the broad theme of music applications. At first we excitedly brainstormed, discussed, and illustrated use cases, user needs, and crazy solutions that addressed these use cases and user needs, ignoring Kinect's limitations. We asked ambitious, broad questions like: How could we augment the physical space of the room the Kinect and its user are in? How could we simultaneously engage and stimulate the user's attention and body? How can we reimagine the way the user interacts with and relates to his/her music? And we also asked simpler yet equally important questions like: what are unmistakable gestures that are not commonly acted out but are easily detected?
			</p>

			<img src="img/whiteboard.jpg">
			<span class="caption">
Possible design layouts and feature sets.  From left to right, top to bottom: text entry, artist images during playback, placing playlists in regions of space, moving tracks from a library to a playlist, pointing for target selection/plus sign to enter edit mode, and x-axis sensitive album navigation similar to iTunes library.
			</span>

			<p>
We identified a recurring problem at social gatherings - who wants to get up to change the music?  Many users play music from computers but continue to interact with attached keyboards and mice, having to get up from a seat or social interaction to add their favorite song to the playlist. Kinect opens up a whole world of natural gestures, allowing users to control the party from the comfort of their couch or in the middle of a conversation. 
			</p>

			<img src="img/notes1.jpg">
			<span class="caption">
We considered the idea of position sensitive playlists, which detect whether your skeletal joints fall into preset or user-defined locations in space.  We also considered ways of engaging the interface, such as tossing selectors at targets, crossing one's arms, or simply pointing.
			</span>

			<img src="img/notes2.jpg">
			<span class="caption">
Brainstorming how to navigate playlists.  Would drilldown menus into subsets of the playlists be effective? How could the user communicate to the interface that he/she loves or hates a track.
			</span>

			<p>
To refine our ideas, we reconsidered them in light of some of the Kinect's limitations, including the fact that it can only actively track the joints of two users and that there was a certain sweet spot for tracking users somewhere between 6' and 9' from the device.  One of the greatest challenges we faced in designing gestures was to design them in a manner that made triggering them accidentally unlikely. Humans are constantly gesturing with their bodies, particularly in crowded social settings where a user would especially want to play music and interact with his/her music library. But over the course of a couple of group meetings during which we brainstormed, sketched, and toyed with the device, we settled on some gestures we all agreed would be effective to choose an item from a collection of items, which we would implement in the next phase of the project.			
			</p>

			<p>
Both of the interactions we designed involve a timeout to select the target, in order to avoid false positives, and one of them requires the user to first prime the device for his/her gestures by raising his/her other hand. Our decision to use these interactions was in part influenced by the Xbox interfaces, which we consider to be the current, most established "best practices" for the Kinect sensor. 
			</p>
		</a>


		<a name="implementation">
			<h1>
III. Implementation
			</h1>

			<p>
After the design phase of our project, we set up our development environments and divided the work to be done. None of us had experience with C#, but greatest hurdle of the implementation phase of our project was just installing Windows 7, Visual Studio 2010, and the Kinect SDK. Many members of our group prefer to use Mac OS X, and these group members had to either partition their drives to install Boot Camp and Windows 7, obtain another computer with Windows 7 on it, or attempt to work from within a VM, which was strongly discouraged both in class and online.
			</p>

			<p>
Once we began coding, our project progressed swiftly. C#'s similarities to Java and the helpful class-provided starter code made it simple to learn to program for the Kinect. We were grateful for the Microsoft's Kinect SDK's built-in capability to track about 20 different joints (some more reliably than others). If we had gone another route and tried to use open-source Kinect SDK's for Mac OS X or Unix distributions, we wouldn't have had this luxury.
			</p>
		
			<iframe src="http://player.vimeo.com/video/35677971?title=0&amp;byline=0&amp;portrait=0" width="680" height="383" frameborder="0" webkitAllowFullScreen mozallowfullscreen allowFullScreen></iframe>
			<span class="caption">
Keegan and Brad demonstrate both of our gesture controllers and also present a simulated use case in which a user can conveniently controls music selection and playback from afar 
			</span>

			<p>Get our source code from <a href="https://github.com/nhippenmeyer/CS247" target="github">https://github.com/nhippenmeyer/CS247</a>, or <a href="https://github.com/nhippenmeyer/CS247/zipball/master">download a .zip directly</a>.</p>
		</a>


		<a name="discussion">
			<h1>
IV. Discussion
			</h1>

			<p>
The two gesture controls we designed and implemented each have unqiue advantages and disadvantages, lending each one to a different use case. The first one only involves waving a single arm, which places very little cognitive strain on the user; the connection between pointing and selecting is clear and intuitive. The second gesture control demands using both hands, which is both physically and cognitively more taxing on the user, but is less likely to trigger false positives. Our video (above) shows a simple use case where users might use our gestures to navigate their music library.
			</p>

			<p>
Using the gestures we designed and implemented to control the Kinect was an exciting experience that further convinced us of the great potential for natural user interfaes. Now that we've learned the ropes, and gotten our feet wet, we can't wait to get down and dirty with Kinect.
			</p>
		</a>

	</div>


	<footer>
		<nav>
			<a href="#introduction">
I. Introduction</a>
|
			<a href="#design_process">
II. Design Process</a>
|
			<a href="#implementation">
III. Implementation</a>
|
			<a href="#discussion">
IV. Discussion</a>
		</nav>

		<nav class="links">
			<a href="http://www.stanford.edu/" target="_">
Stanford University</a>
/
			<a href="http://hci.stanford.edu/courses/cs247/2012/" target="_">
Computer Science 247: Human-Computer Interaction Design Studio</a>
/
			<a href="http://hci.stanford.edu/courses/cs247/2012/p3.html" target="_">
Project 3: Gesture Controller</a>
		</nav>

Last modified January 26th, 2012
	</footer>

</body>
</html>

